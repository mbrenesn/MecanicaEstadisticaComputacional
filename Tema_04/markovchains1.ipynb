{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de cadenas de Markov\n",
    "\n",
    "En nuestras lecciones anteriores, hemos estudiado procesos estocásticos en los cuales el estado del sistema **es independiente de su historia**.\n",
    "- Considere el camino aleatorio: a pesar de que, e.g., el desplazamiento total depende de la trayectoria anterior, el sistema se mueve con una distribución de probabilidad de pasos en los cuales el paso futuro no depende del estado actual.\n",
    "- ¿Qué pasa si elevamos este desarrollo, de manera tal que la probabilidad de moverse a un estado depende de los estados anteriores?  \n",
    "\n",
    "Las **cadenas de Markov** constituyen una de las estructuras matemáticas más poderosas y versátiles para modelar sistemas aleatorios que evolucionan en el tiempo (procesos estocásticos). Este desarrollo es importante en varios aspectos del conocimiento, sin embargo, su relevancia para nosotros se basa principalmente en los siguientes aspectos:\n",
    "\n",
    "- **En física estadística**, describen la evolución estocástica de sistemas microscópicos que fluctúan térmicamente (como espines, partículas o configuraciones energéticas) y son la base de los algoritmos de **Monte Carlo tipo cadenas de Markov (MCMC)** empleados para estimar propiedades de equilibrio.\n",
    "- **En matemáticas**, proporcionan una formulación rigurosa para estudiar procesos estocásticos con dependencia local, permitiendo analizar la convergencia hacia estados estacionarios, la estabilidad, la ergodicidad y los tiempos de mezcla.\n",
    "- **En computación científica**, son la base de algoritmos de muestreo y optimización en alta dimensión, utilizados en física computacional, inferencia bayesiana, aprendizaje automático y simulación molecular.\n",
    "\n",
    "La idea central de una cadena de Markov es que el futuro de un sistema depende únicamente de su estado presente, **no de cómo llegó hasta él**. Esta propiedad, conocida como **propiedad de Markov**, simplifica drásticamente la descripción de sistemas complejos.\n",
    "\n",
    "En este sentido, las cadenas de Markov elevan el concepto del camino aleatorio que hemos estudiado. \n",
    "- Desde esta perspectiva, un camino aleatorio es un caso especial de una cadena de Markov.\n",
    "\n",
    "#### Desde la perspectiva física\n",
    "\n",
    "En física estadística, muchas cantidades de interés como la energía promedio, la magnetización o las correlaciones espaciales son **promedios de equilibrio** calculados sobre el conjunto de microestados accesibles a un sistema:\n",
    "$$\n",
    "E[f(X)] \\equiv \\langle f(x) \\rangle = \\sum_{x \\in \\mathcal{S}} f(x)\\, \\pi(x)\n",
    "$$\n",
    "donde $\\pi(x)$ es la **distribución de equilibrio** (por ejemplo, la distribución de Boltzmann):\n",
    "$$\n",
    "\\pi(x) = \\frac{1}{Z} e^{-\\beta E(x)}, \\quad Z = \\sum_{x \\in \\mathcal{S}} e^{-\\beta E(x)}\n",
    "$$\n",
    "\n",
    "El problema es que el espacio de estados $\\mathcal{S}$ puede ser enorme (crece exponencialmente con respecto a alguna variable de configuración), lo que impide calcular directamente estos promedios en muchos casos prácticos.\n",
    "\n",
    "El enfoque de **Monte Carlo tipo cadenas de Markov (MCMC)** construye una secuencia de configuraciones $X_0, X_1, X_2, \\ldots$ de tal forma que los estados se visitan con la probabilidad deseada $\\pi(x)$.  \n",
    "Así, el promedio temporal aproxima el promedio de equilibrio:\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{n=1}^{N} f(X_n) \\approx \\sum_{x \\in \\mathcal{S}} f(x)\\, \\pi(x)\n",
    "$$\n",
    "\n",
    "El fundamento teórico de esta equivalencia es la **ergodicidad** de la cadena.\n",
    "\n",
    "#### Desde la perspectiva matemática\n",
    "\n",
    "Desde el punto de vista matemático, una cadena de Markov es una **sucesión de variables aleatorias** $\\{X_n\\}_{n \\ge 0}$ que toman valores en un conjunto finito o numerable de estados $\\mathcal{S}$ y satisfacen:\n",
    "$$\n",
    "P\\{X_{n+1} = j \\mid X_n = i, X_{n-1}, \\ldots, X_0\\} = P\\{X_{n+1} = j \\mid X_n = i\\}\n",
    "$$\n",
    "\n",
    "El comportamiento de la cadena se describe completamente por su **matriz de transición** $P = (P_{ij})$, donde:\n",
    "$$\n",
    "P_{ij} = P\\{X_{n+1} = j \\mid X_n = i\\}, \\quad\n",
    "P_{ij} \\ge 0, \\quad \\sum_{j} P_{ij} = 1\n",
    "$$\n",
    "\n",
    "A partir de $P$, se pueden estudiar propiedades fundamentales como:\n",
    "\n",
    "- **Comportamiento transitorio:** cómo evoluciona la distribución de estados en el tiempo.\n",
    "- **Distribución estacionaria:** $\\pi = \\pi P$, que representa el equilibrio del sistema.\n",
    "- **Ergodicidad y mezcla:** condiciones bajo las cuales $\\pi^{(n)} \\to \\pi$ conforme $n \\to \\infty$.\n",
    "\n",
    "Estas propiedades son esenciales para garantizar que las simulaciones de Monte Carlo generen muestreos estadísticamente representativos del equilibrio físico.\n",
    "\n",
    "#### Desde la perspectiva computacional\n",
    "\n",
    "En computación, las cadenas de Markov proporcionan una estrategia práctica para **muestrear** espacios de alta dimensión sin recorrerlos exhaustivamente.\n",
    "\n",
    "En lugar de generar configuraciones independientes, se construye una cadena que las visita de acuerdo con la distribución deseada $\\pi(x)$.  \n",
    "Este enfoque tiene varias ventajas:\n",
    "\n",
    "- **Escalabilidad:** se adapta a sistemas con millones de grados de libertad.\n",
    "- **Flexibilidad:** se puede aplicar en contextos donde no se conoce la forma explícita de $\\pi(x)$, siempre que se pueda evaluar proporcionalmente.\n",
    "- **Generalidad:** los métodos MCMC (Metropolis-Hastings, Gibbs Sampling, etc.) pueden aplicarse en dominios continuos y discretos.\n",
    "\n",
    "Así, las cadenas de Markov constituyen un **lenguaje unificador** entre la teoría de la probabilidad, la física estadística y la simulación computacional moderna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Introduciremos las **cadenas de Markov** como base teórica de los **métodos de Monte Carlo basados en cadenas de Markov (MCMC)**.  \n",
    "El objetivo es comprender cómo procesos estocásticos con dependencia local pueden generar secuencias de muestras que convergen a una distribución de equilibrio deseada.\n",
    "\n",
    "Consideraremos para efectos de esta lecciones, procesos de Markov de espacio y tiempo discretos.\n",
    "\n",
    "### Procesos de Markov y la propiedad de memoria corta\n",
    "\n",
    "El camino aleatorio es un ejemplo de un **proceso estocástico**, i.e., un proceso en el cual la variable aleatorio cambia con respecto a un parámetro. De ahora en adelante, le llamaremos a ese parámetro **tiempo**.\n",
    "\n",
    "Sea $X_t$ el *estado del sistema al tiempo $t$*. \n",
    "- Para el camino aleatorio, este estado es relativamente sencillo: $X_t$ está compuesto de variables aleatorias I.I.D. Los pasos son independientes del estado actual.\n",
    "- Las cadenas de Markov elevan este concepto, de manera tal que el estado en el futuro depende de la información acerca del estado actual.\n",
    "- Para el caso de cadenas de Markov discretas en espacio y tiempo, el tiempo es una variable discreta y los valores posibles de las variables aleatorias que constituyen al estado también son discretos.\n",
    "- El sistema evoluciona en el tiempo mediante trayectorias estocásticas (realizaciones de un proceso estocástico)\n",
    "\n",
    "La suposición básica de las cadenas de Markov se conoce como la **propiedad de Markov**:\n",
    "$$\n",
    "\\boxed{P\\{ X_{t+1} = m_{t + 1} | X_t = m_t, X_{t-1} = m_{t-1}, \\cdots, X_0 = m_0 \\} = P\\{ X_{t+1} = m_{t + 1} | X_t = m_t\\}.}\n",
    "$$\n",
    "- Pensando en $t$ como el tiempo, la propiedad indica que el **futuro depende solamente del estado actual y no de eventos pasados**.\n",
    "- Decimos que los sistemas Markovianos tienen **memoria corta**.\n",
    "\n",
    "Las cantidades\n",
    "$$\n",
    "\\boxed{Q_{nm} = P\\{ X_{t+1} = n | X_t = m \\} = {\\textrm{Probabilidad}}(m \\to n)}\n",
    "$$\n",
    "se conocen como las **probabilidades de transición**. Representan la probabilidad de observar una transición de $m$ a $n$.\n",
    "\n",
    "Supongamos que conocemos $P\\{ X_t = m \\}$. Podemos encontrar $P\\{ X_{t+1} = n \\}$ mediante el axioma de la probabilidad\n",
    "\\begin{align}\n",
    "P\\{ X_{t+1} = n \\} &= \\sum_m P\\{ X_{t+1} = n | X_t = m \\} P\\{ X_t = m \\} \\\\\n",
    "&= \\sum_m Q_{nm} P\\{ X_t = m \\}.\n",
    "\\end{align}\n",
    "Usando la notación corta $P_n(t) = P\\{ X_t = n \\}$, podemos escribir la ecuación anterior como\n",
    "$$\n",
    "\\boxed{P_n(t+1) = \\sum_m Q_{nm} P_m(t).}\n",
    "$$\n",
    "- Esta es la ecuación fundamental de la cadena de Markov.\n",
    "- Note que es una **ecuación dinámica**. Corresponde a una receta de cómo evolucionar el sistema en el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es difícil notar que la ecuación anterior tiene la estructura de una multiplicación matricial. Definimos el vector columna\n",
    "$$\n",
    "\\vec{P}(t) = \\begin{pmatrix} P_1(t) \\\\ P_2(t) \\\\ \\vdots \\\\ P_n(t) \\end{pmatrix}\n",
    "$$\n",
    "y la matrix $\\mathbf{Q}$ con entradas $Q_{nm}$. Luego\n",
    "$$\n",
    "\\vec{P}(t+1) = \\mathbf{Q} \\vec{P}(t),\n",
    "$$\n",
    "donde $\\mathbf{Q}$ corresponde a la **matriz de transición**.\n",
    "Si empezamos en el tiempo $t = 0$ con la distribución $\\vec{P}(0)$, entonces \n",
    "$$\n",
    "\\vec{P}(1) = \\mathbf{Q}\\vec{P}(0)\n",
    "$$\n",
    "y adicionalmente\n",
    "$$\n",
    "\\vec{P}(2) = \\mathbf{Q}\\vec{P}(1) = \\mathbf{Q}^2\\vec{P}(0),\n",
    "$$\n",
    "de manera tal que \n",
    "$$\n",
    "[\\mathbf{Q}^2]_{nm} = P\\{ X_{t + 2} = n | X_t = m \\}.\n",
    "$$\n",
    "Llamamos a $X_0$, $X_1$, $X_2$, ... una **cadena de Markov**. En general\n",
    "$$\n",
    "\\boxed{\\vec{P}(t) = \\mathbf{Q}^t \\vec{P}(0)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo: Clima 1\n",
    "\n",
    "Considere el ejemplo sencillo siguiente. El clima puede estar soleado (S) o lluvioso (L). Si estea soleado hoy, existe una probabilidad $0.2$ de que esté lluvioso mañana (presumiblemente por el agua que se evapora). Si está lluvioso hoy, existe una probabilidad $0.4$ de que esté soleado mañana. \n",
    "- En general, el clima no satisface la propiedad de Markov dado que en general la memoria en este sistema no es corta.\n",
    "- Estamos usando este ejemplo de forma reduccionista para visualizar los procesos de Markov\n",
    "\n",
    "![Figura 1](Fig1.pdf \"Figura 1\")\n",
    "\n",
    "- Los procesos de Markov se pueden representar usando **grafos**, ver la figura anterior.\n",
    "\n",
    "Para este sistema de dos estados, usemos $1=$Soleado y $2=$Lluvioso, de manera tal que nuestro sistema se representa con un vector\n",
    "$$\n",
    "\\vec{P}(t) = \\begin{pmatrix} P\\{ X_t = S \\} \\\\ P\\{ X_t = L \\} \\end{pmatrix} = \\begin{pmatrix} P_1(t) \\\\ P_2(t) \\end{pmatrix}\n",
    "$$\n",
    "Usando el segundo axioma de probabilidad, tenemos que\n",
    "\\begin{align}\n",
    "P\\{ X_{t+1} = S \\} &= P\\{ X_{t+1} = S | X_t = S \\} P\\{ X_t = S \\} + P\\{ X_{t+1} = S | X_t = R \\} P\\{ X_t = R \\} \\\\\n",
    "P\\{ X_{t+1} = R \\} &= P\\{ X_{t+1} = R | X_t = S \\} P\\{ X_t = S \\} + P\\{ X_{t+1} = R | X_t = R \\} P\\{ X_t = R \\},\n",
    "\\end{align}\n",
    "usando nuestro grafo de probabilidades obtenemos\n",
    "\\begin{align}\n",
    "P_1(t+1) &= 0.8 P_1(t) + 0.4 P_2(t) \\\\\n",
    "P_2(t+1) &= 0.2 P_1(t) + 0.6 P_2(t).\n",
    "\\end{align}\n",
    "De esto obtenemos directamente la matrix de transición\n",
    "$$\n",
    "\\mathbf{Q} = \\begin{pmatrix} 0.8 & 0.4 \\\\ 0.2 & 0.6 \\end{pmatrix}.\n",
    "$$\n",
    "Suponga que tenemos el estado inicial de que el día de hoy está soleado. Es decir\n",
    "$$\n",
    "\\vec{P}(0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\n",
    "$$\n",
    "de manera tal que la distribución para el día de mañana sería\n",
    "$$\n",
    "\\vec{P}(1) = \\mathbf{Q} \\vec{P}(0) = \\begin{pmatrix} 0.8 & 0.4 \\\\ 0.2 & 0.6 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.8 \\\\ 0.2 \\end{pmatrix} \n",
    "$$\n",
    "y la distribución para el día después de mañana sería\n",
    "$$\n",
    "\\vec{P}(2) = \\mathbf{Q} \\vec{P}(1) = \\begin{pmatrix} 0.8 & 0.4 \\\\ 0.2 & 0.6 \\end{pmatrix} \\begin{pmatrix} 0.8 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 0.72 \\\\ 0.28 \\end{pmatrix} \n",
    "$$\n",
    "y así sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinar la matriz de transición mediante un grafo\n",
    "\n",
    "Para determinar la matriz de transición de un grafo, es útil recordar que\n",
    "$$\n",
    "Q_{nm} = P(m \\to n),\n",
    "$$\n",
    "de manera tal que se leen las entradas de forma inversa. Considere, por ejemplo, el siguiente grafo:\n",
    "![Figura 2](Fig2.pdf \"Figura 2\")\n",
    "\n",
    "En este caso, la matrix de transición sería\n",
    "$$\n",
    "\\mathbf{Q} = \\begin{pmatrix} 1/3 & 1/2 & 0 & 1/2 \\\\ 2/3 & 0 & 0 & 0 \\\\ 0 & 1/2 & 0 & 1/4 \\\\ 0 & 0 & 1 & 1/4 \\end{pmatrix}\n",
    "$$\n",
    "Note que la suma de cada columna debe ser $1$ para satisfacer el axioma de probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices estocásticas\n",
    "\n",
    "La matriz de transición $Q$ satisface algunas propiedades muy especiales. \n",
    "- Sus entradas son no negativas, lo cual es obvio debido a la interpretación probabilística.\n",
    "$$\n",
    "Q_{nm} = P\\{ X_t = n | X_{t - 1} = m \\} \\geq 0 \n",
    "$$\n",
    "- Las columnas de $Q$ suman a 1.\n",
    "$$\n",
    "\\sum_{n} Q_{nm} = 1 \\quad \\forall m\n",
    "$$\n",
    "De esta forma, el proceso **conserva probabilidad**.\n",
    "- Una matriz que satisface las dos condiciones anteriores se conoce como una **matriz estocástica**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo: Clima 2\n",
    "\n",
    "En nuestro ejemplo anterior, podemos observar un efecto interesante. Al considerar tiempos sucesivos, obtenemos\n",
    "$$\n",
    "\\vec{P}(t) : \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}_{t = 0}, \\begin{pmatrix} 0.8 \\\\ 0.2 \\end{pmatrix}_{t = 1}, \\begin{pmatrix} 0.72 \\\\ 0.28 \\end{pmatrix}_{t = 2}, \\begin{pmatrix} 0.688 \\\\ 0.312 \\end{pmatrix}_{t = 3}, \\cdots, \\begin{pmatrix} 0.666667 \\\\ 0.333333 \\end{pmatrix}_{t = 1000}.\n",
    "$$\n",
    "Vemos que conforme el tiempo avanza, las probabilidades convergen a ciertos valores. \n",
    "- A esta distribución le llamamos **la distribución del estado estacionario**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado estacionario\n",
    "\n",
    "Cuando el sistema alcanza el estado estacionario, evolucionar a tiempos mayores no cambia la distribución del sistema, i.e., el estado del sistema. Si denotamos $\\vec{P}^*$ como el estado estacionario, entonces se satisface que\n",
    "$$\n",
    "\\boxed{\\mathbf{Q} \\vec{P}^* = \\vec{P}^*.}\n",
    "$$\n",
    "Esta es la ecuación que determina el estado estacionario. \n",
    "- Note que esta ecuación tiene la forma de una **ecuación de autovalores**.\n",
    "- De esta forma, **el estado estacionario corresponde al autovector de $\\mathbf{Q}$ con autovalor 1**.\n",
    "- En general, encontrar el estado estacionario se reduce a resolver para este autovalor/autovector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo: Clima 3\n",
    "\n",
    "Encontremos la distribución del estado estacionario en nuestro ejemplo sencillo del clima. Para esto, resolvemos la ecuación\n",
    "\\begin{align}\n",
    "{\\textrm{det}}(\\mathbf{Q} - \\lambda \\mathbf{I}) &= {\\textrm{det}} \\begin{bmatrix} \\frac{4}{5} - \\lambda & \\frac{2}{5} \\\\ \\frac{1}{5} & \\frac{3}{5} - \\lambda \\end{bmatrix} \\\\\n",
    "&= \\Big(\\frac{4}{5} - \\lambda \\Big) \\Big(\\frac{3}{5} - \\lambda \\Big) - \\frac{2}{5} \\cdot \\frac{1}{5} \\\\\n",
    "&= \\lambda^2 - \\frac{7}{5} \\lambda + \\frac{2}{5}.\n",
    "\\end{align}\n",
    "Los autovalores son\n",
    "$$\n",
    "\\lambda_1 = 1, \\quad \\lambda_2 = \\frac{2}{5}\n",
    "$$\n",
    "El autovector con el autovalor $\\lambda_1 = 1$ se obteiene directamente\n",
    "$$\n",
    "\\frac{1}{5} \\begin{pmatrix} 4 & 2 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = (1) \\begin{pmatrix} a \\\\ b \\end{pmatrix},\n",
    "$$\n",
    "lo cual implica que \n",
    "\\begin{align}\n",
    "\\frac{4a}{5} + \\frac{2b}{5} = a \\\\\n",
    "\\implies a = 2b.\n",
    "\\end{align}\n",
    "Finalmente\n",
    "$$\n",
    "\\vec{P}^* = b \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.\n",
    "$$\n",
    "La constante $b$ es determinada mediante la condición de normalización $2b + b = 1$, tal que $b = 1 / 3$. Luego\n",
    "$$\n",
    "\\boxed{\\vec{P}^* = \\begin{pmatrix} 2/3 \\\\ 1/3 \\end{pmatrix}.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condiciones acerca del estado estacionario\n",
    "\n",
    "El estado estacionario corresponde al aspecto más importante de una cadena de Markov. Dada una cadena específica con una matriz de transición $\\mathbf{Q}$, queremos saber los siguientes aspectos acerca de $\\vec{P}^*$:\n",
    "- ¿Existe?\n",
    "- ¿Es único?\n",
    "- ¿La cadena llega a alcanzar $\\vec{P}^*$?\n",
    "- ¿Existe dependencia de la condición inicial $\\vec{P}(0)$?\n",
    "\n",
    "#### Clasificación de los estados\n",
    "\n",
    "Para responder a las preguntas anteriores, debemos analizar la estructura de las cadenas de Markov:\n",
    "\n",
    "- **Comunicabilidad:** decimos que $n$ *se comunica con* $m$ (notación $n \\leftrightarrow m$) si existen enteros $i, j$ tales que $P^i_{nm} > 0$ y $P^j_{mn} > 0$. Esta es una **relación de equivalencia** que particiona el espacio de estados en *clases de comunicación*.\n",
    "- **Irreducibilidad:** una cadena es **irreducible** si todos los estados se comunican, es decir, el espacio de estados forma una sola clase.\n",
    "- **Periodo:** el *periodo* de un estado $n$ se define como el máximo común divisor de los enteros $i \\ge 1$ tales que $P^i_{nn} > 0$. Si $d(i) = 1$, el estado es **aperiódico**.\n",
    "- **Recurrente/transitorio:** un estado es **recurrente** si la probabilidad de retornar eventualmente a él es 1, y **transitorio** en caso contrario.\n",
    "\n",
    "Las preguntas anteriores se pueden enfrentar matemáticamente para **cadenas irreducibles**.\n",
    "- Una cadena es irreducible si es posible (en notación de grafo) **ir de cualquier lugar a otro**.\n",
    "- Existen dos casos en los cuales no se alcanza la irreducibilidad:\n",
    "  1) Cuando el proceso de Markov se describe mediante grafos desconectados.\n",
    "  2) Cuando existen *estados absorbentes*, i.e., un camino siempre lleva a un nodo del grafo sin probabilidad de salir. \n",
    "- Dada una matriz de transición $\\mathbf{Q}$, el criterio de irreducibilidad es tal que\n",
    "    * Una cadena de Markov es irreducible si para cada $(n, m)$, existe un entero $j$ tal que $(Q^j)_{nm} > 0$.\n",
    "    * Esto significa que después de $j$ pasos temporales, es posible alcanzar $n$ partiendo de $m$.\n",
    "\n",
    "La mayoría de teoremas con respecto al estado estacionario se refieren a cadenas irreducibles.\n",
    "\n",
    "Los únicos procesos que son irreducibles y problemáticos son los **procesos cíclicos o periódicos**, en los cuales las matrices de transición $\\mathbf{Q}$ son **irregulares**. Estos casos son singulares en procesos de Markov y la convergencia a un estado estacionario no se puede asegurar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teoremas sobre el estado estacionario para cadenas irreducibles\n",
    "\n",
    "**Teorema (existencia).**  \n",
    "Sea $\\mathbf{Q}$ la matriz de transición de una cadena de Markov con espacio de estados finito $\\mathcal S = \\{1,\\dots,m\\}$. Entonces existe al menos un vector fila $\\vec{P}^*$ con componentes $P^*_i \\ge 0$ y $\\sum_{i=1}^m P_i^* = 1$ tal que\n",
    "$$\n",
    "\\vec{P}^* = \\mathbf{Q} \\vec{P}^*.\n",
    "$$\n",
    "\n",
    "**Teorema (unicidad y convergencia).**  \n",
    "Si la cadena de Markov con matriz de transición $\\mathbf{Q}$ es **irreducible** y **aperiódica (acíclica)**, entonces:\n",
    "\n",
    "1. Existe una **única** distribución estacionaria $\\vec{P}^*$ con $P^*_i > 0$ para todo $i$.\n",
    "2. Para cualquier distribución inicial $\\vec{P}(0)$ y para todos los estados $j$,\n",
    "   $$\n",
    "   \\lim_{n\\to\\infty} \\mathbf{Q}^n \\vec{P}(0) = \\vec{P}^*,\n",
    "   $$\n",
    "\n",
    "No detallaremos la prueba de estos teoremas, pero se derivan del teorema de Perron-Forbenius para matrices no negativas. Adicionalmente, las siguientes propiedades se satisfacen para cadenas irreversibles y aperiódicas:\n",
    "- La matriz $\\mathbf{Q}$ tiene solamente un autovalor igual a 1.\n",
    "- El resto de los autovalores satisfacen $|\\lambda| < 1$.\n",
    "- El autovector correspondiente a $\\lambda = 1$ es único y todas sus entradas son positivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance detallado y balance global\n",
    "\n",
    "La ecuación de estado estacionario es $\\mathbf{Q} \\vec{P}^{*} = \\vec{P}^{*}$. En notación de componentes esto se puede escribir como\n",
    "$$\n",
    "\\sum_{m} Q_{nm} P^*_m = P^*_n.\n",
    "$$\n",
    "Esto se puede representar de forma simétrica insertando el valor 1 en la parte derecha de la ecuación, sabiendo que\n",
    "$$\n",
    "1 = \\sum_{m} Q_{mn},\n",
    "$$\n",
    "tenemos que\n",
    "$$\n",
    "\\boxed{\\sum_m Q_{nm} P^*_m = \\sum_m Q_{mn} P^*_n.}\n",
    "$$\n",
    "A esta condición se le llama **relación de balance global**. Indica que el estado estacionario es el estado para el cual todas las transiciones han sido balanceadas. \n",
    "\n",
    "Algunos sistemas, particularmente sistemas que aparecen en física, satisfacen una condición más extraña conocida como **balance detallado**. En este caso, no solo la suma del lado derecho y la suma del lado izquierdo son equivalentes, sino que sus **componentes individuales también son iguales**:\n",
    "$$\n",
    "\\boxed{Q_{nm} P^*_m = Q_{mn} P^*_n.}\n",
    "$$\n",
    "Esta condición no es ubicua, pero es muy común en sistemas físicos. En general, la condición del balance detallado emerge de la invariancia de inversión temporal. \n",
    "- **La existencia del balance detallado implica la presencia de un estado estado estacionario.**\n",
    "- Balance detallado es una condición **suficiente**, pero no necesaria.\n",
    "\n",
    "Esto se puede probar de la siguiente forma.\n",
    "\n",
    "Para cada estado $j$,\n",
    "\\begin{align*}\n",
    "(Q \\vec{P}^*)_j &= \\sum_{i} Q_{ji} P_{i}^* \\\\\n",
    "&= \\sum_{i} Q_{ij} P_{j}^* \\qquad\\text{(por balance detallado)}\\\\\n",
    "&= P_{j}^* \\sum_{i} Q_{ij} \\\\\n",
    "&= P_{j}^*,\n",
    "\\end{align*}\n",
    "Lo cual implica la existencia de un estado estacionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Sistema de dos niveles\n",
    "\n",
    "Estudiemos una cadena de Markov discreta con dos estados (niveles de energía) que modela la transición térmica entre dos niveles $E_0$ y $E_1$. Verificaremos por simulación la distribución estacionaria teórica.\n",
    "\n",
    "Los estados son $\\mathcal{S}=\\{0,1\\}$ con energías $E_0$ y $E_1$ respectivamente. Definimos las probabilidades de transición (discretas en tiempo):\n",
    "- Si $X_n = 0$:\n",
    "  $$\n",
    "  P\\{X_{n+1}=1 | X_n=0 \\} = p, \\qquad P\\{ X_{n+1}=0\\mid X_n=0 \\} = 1-p.\n",
    "  $$\n",
    "- Si $X_n = 1$:\n",
    "  $$\n",
    "  P\\{ X_{n+1}=0\\mid X_n=1 \\} = q, \\qquad P\\{ X_{n+1}=1\\mid X_n=1 \\} = 1-q.\n",
    "  $$\n",
    "\n",
    "1) Encuentre la matriz de transición.\n",
    "2) Encuentre el estado estacionario (esto se puede hacer con la condición $\\vec{P}^* = \\mathbf{Q} \\vec{P}^*$ o aplicando balance detallado).\n",
    "3) Aplique la distribución de Boltzmann para hacer que nuestros valores de $p$ y $q$ coincidan con la distribución canónica.\n",
    "La distribución canónica a temperatura $T$ (constante de Boltzmann $k_B$) es:\n",
    "$$\n",
    "P^*_i \\propto e^{-\\beta E_i},\\qquad \\beta=\\frac{1}{k_B T}.\n",
    "$$\n",
    "Entonces\n",
    "$$\n",
    "\\frac{P^*_1}{P^*_0} = e^{-\\beta (E_1-E_0)} \\equiv e^{-\\beta\\Delta E}.\n",
    "$$\n",
    "Utilice esta distribución para determinar las condiciones sobre $p$ y $q$.\n",
    "4) Escriba una rutina que dado $\\beta$ y $\\delta E$, devuelve los valores de $p$ y $q$.\n",
    "5) Utilice las siguientes rutinas para simular la cadena de Markov y estudiar las frecuencias para ver si calzan con la distribución teórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_chain(p, q, N, x0=0):\n",
    "    \"\"\"Simula N pasos de la cadena discreta en {0,1} con transiciones p,q.\"\"\"\n",
    "    x = np.empty(N, dtype=int)\n",
    "    x[0] = x0\n",
    "    for n in range(1, N):\n",
    "        u = np.random.random()\n",
    "        if x[n-1] == 0:\n",
    "            # probabilidad de ir a 1 = p\n",
    "            x[n] = 1 if u < p else 0\n",
    "        else:\n",
    "            # estado 1 -> va a 0 con prob q\n",
    "            x[n] = 0 if u < q else 1\n",
    "    return x\n",
    "\n",
    "def empirical_freq(x):\n",
    "    \"\"\"Devuelve frecuencias empíricas.\"\"\"\n",
    "    n = x.size\n",
    "    pi1 = np.mean(x == 1)\n",
    "    return 1 - pi1, pi1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

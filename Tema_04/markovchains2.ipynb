{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergodicidad y el método de Metropolis-Hastings\n",
    "\n",
    "En nuestra lección anterior:\n",
    "- Introducimos las cadenas de Markov como una generalización a la teoría de probabilidad con variables aleatorias dependientes.\n",
    "- Las cadenas de Markov poseen la propiedad de **memoria corta**.\n",
    "- Se pueden representar usando grafos.\n",
    "- El objeto de mayor importancia es la **matriz de transición $\\mathbf{Q}$**, la cual describe la cadena de Markov.\n",
    "- Obtuvimos una **ecuación dinámica** de la evolución de los posibles estados de una cadena de Markov, la cual **preserva la probabilidad**.\n",
    "- Estudiamos los **estados estacionarios**; los cuales existen, son únicos y convergen dependiendo de ciertas condiciones:\n",
    "    * Lo anterior se cumple para cadenas **irreducibles y aperiodícas** (positivas recurrentes).\n",
    "    * El balance global es una condición necesaria, pero no suficiente, para un estado estacionario con respecto a su **existencia**.\n",
    "    * El balance detallado es una condición suficiente, pero no necesaria, para un estado estacionario con respecto a su **existencia**.\n",
    "\n",
    "En esta lección daremos un paso más: construiremos **métodos generales de muestreo** basados en cadenas de Markov que permiten **simular distribuciones de probabilidad complejas**, incluso cuando no podemos escribir ni normalizar $\\vec{P}^*$ de forma analítica.\n",
    "\n",
    "El método más conocido y poderoso en este contexto es el **algoritmo de Metropolis–Hastings**, base de los métodos **MCMC (Markov Chain Monte Carlo)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ergodicidad y equilibrio en cadenas de Markov\n",
    "\n",
    "Cuando se cumple la condición de balance detallado\n",
    "$$\n",
    "Q_{nm} P^*_m = Q_{mn} P^*_n,\n",
    "$$\n",
    "decimos que la cadena de Markov es **reversible temporal**. \n",
    "- Bajo esta condición se puede probar que si un estado inicial se escoje de acuerdo con $P^*_n$, entonces la secuencia de estados moviéndose hacia atrás en el tiempo también serán una cadena de Markov con probabilidades de transición $Q_{nm}$.\n",
    "\n",
    "Ahora hablaremos sobre una **condición suficiente para la unicidad y convergencia de un estado estacionario para una cadena de Markov**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $\\{X_t\\}_{t \\ge 0}$ una cadena de Markov en un espacio de estados $\\mathcal{S}$ (finito o numerable), con matriz de transición $Q_{nm} = P\\{ X_{t+1} = n | X_t = m \\}$.\n",
    "\n",
    "Si definimos $P_n(t) = P \\{ X_t = n \\}$, sabemos que\n",
    "$$\n",
    "P_n(t + 1) = \\sum_n Q_{nm} P_m(t) \\implies \\vec{P}(t + 1) = \\mathbf{Q} \\vec{P}(t).\n",
    "$$\n",
    "Sabemos que si\n",
    "$$\n",
    "\\vec{P}(0) \\sim \\vec{P}^*\n",
    "$$\n",
    "entonces se tiene que cumplir por construcción que para todo $t$\n",
    "$$\n",
    "\\vec{P}(t) \\sim \\vec{P}^*,\n",
    "$$\n",
    "es decir, $\\vec{P}^*$ es **invariante bajo la dinámica de la cadena**. Esto es lo que implica que $\\vec{P}^*$ sea una distribución (o estado, dependiendo de la literatura) estacionario, lo cual es equivalente a $\\vec{P}^* = \\mathbf{Q} \\vec{P}^*$.\n",
    "\n",
    "- Adicionalmente, para toda distribución inicial $\\vec{W}$, si la distribución $\\mathbf{Q}^t \\vec{W}$ **converge** a $\\vec{P}^*$ cuando $t \\to \\infty$ entonces decimos que que $\\vec{P}^*$ es **la distribución (o estado) de equilibrio**.\n",
    "\n",
    "Formalmente,\n",
    "$$\n",
    "\\lim_{t\\to\\infty} \\|\\mathbf{Q}^t \\vec{W} - \\vec{P}^* \\|_{\\text{TV}} = 0,\n",
    "$$\n",
    "donde $\\|\\cdot\\|_{\\text{TV}}$ es la distancia total de variación. Note que, en general, esto no corresponde a la norma de la diferencia de dos vectores, dado que los vectores estrictamente hablando representan **distribuciones de probabilidad**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condiciones para la ergodicidad\n",
    "\n",
    "Decimos que una cadena es **ergódica** si cumple tres condiciones fundamentales:\n",
    "\n",
    "1. **Irreducibilidad:**  \n",
    "   Para todo $n, m \\in \\mathcal{S}$, existe un $t$ tal que $(\\mathbf{Q}^t)_{nm} > 0$.  \n",
    "   Es decir, todos los estados se comunican: cualquier estado puede alcanzarse eventualmente desde cualquier otro.\n",
    "\n",
    "2. **Aperiodicidad:**  \n",
    "   El periodo de un estado $n$ con $i \\geq 1$ se define como  \n",
    "   $$\n",
    "   d(i) = {\\textrm{mcd}}\\{ n \\ge 1 : (\\mathbf{Q}^i)_{nn} > 0 \\}.\n",
    "   $$\n",
    "   La cadena es aperiódica si $d(i) = 1$ para todo $n$ (o al menos para un estado del que todos los demás sean accesibles).\n",
    "\n",
    "3. **Existencia de distribución estacionaria:**  \n",
    "   Debe existir $\\vec{P}^*$ con $\\vec{P}^* = \\mathbf{Q} \\vec{P}^*$ y $\\sum_i P^*_i = 1$.\n",
    "\n",
    "Cuando estas tres propiedades se cumplen simultáneamente, la cadena es **ergódica** y se cumple el **teorema de convergencia ergódica**.\n",
    "\n",
    "Ross (Simulation, 5th Ed.) da una definición adicional de una cadena de Markov irreducible y aperiódica más sencilla (menos matemática):\n",
    "#### Definifición:\n",
    "Se dice que una cadena de Markov es irreducible y aperiódica si para algún $n \\geq 0$ y algún estado $j$,\n",
    "$$\n",
    "\\boxed{P\\{ X_n = j | X_0 = j \\} > 0 \\quad {\\textrm{y}}\\quad P\\{ X_{n+1} = j | X_0 = j \\} > 0.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teorema ergódico para cadenas de Markov\n",
    "\n",
    "Sea $\\{X_t\\}_{t \\geq 0}$ una cadena de Markov irreducible, aperiódica, con distribución estacionaria $\\vec{P}^*$.  \n",
    "Entonces, para cualquier función acotada $f:\\mathcal{S}\\to\\mathbb{R}$,\n",
    "\n",
    "$$\n",
    "\\lim_{T\\to\\infty} \\frac{1}{T}\\sum_{t=1}^T f(\\vec{P}(t)) = E[f(\\vec{P}^*)],\n",
    "$$\n",
    "\n",
    "con probabilidad 1.\n",
    "\n",
    "Esto significa que los **promedios temporales** (evaluados a lo largo de la cadena) convergen a los **promedios espaciales** (con respecto a $\\vec{P}^*$).  \n",
    "\n",
    "En física, este resultado **justifica que una simulación suficientemente larga puede reemplazar al promedio de ensamble sobre todas las configuraciones posibles.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expansión sobre la base de autovectores\n",
    "\n",
    "La prueba del teorema utiliza conceptos de la teoría de probabilidad más avanzados de los que hemos estudiado hasta el momento, así que no realizaremos la prueba de forma explícita.\n",
    "\n",
    "Con base en la irreducibilidad y aperiodicidad de una cadena de Markov, esta condición se traduce en la teoría de matrices a que la matriz de transición $\\mathbf{Q}$ es **primitiva**, lo cual implica que alguna potencia de $\\mathbf{Q}$ tiene todas las entradas *estrictamente positivas*. El teorema de Perron-Frobenius indica que para matrices primitivas para un espacio de muestreo $m \\in \\mathcal{S}$: \n",
    "- La matriz $\\mathbf{Q}$ tiene solamente un autovalor igual a $\\lambda_1 = 1$.\n",
    "- Todos los otros $m - 1$ autovalores satisfacen $|\\lambda_i| < 1$, $i = 2, 3, \\cdots, m$.\n",
    "- El autovector correspondiente a $\\lambda_1 = 1$ es único y todas sus entradas son positivas (corresponde a $\\vec{P}^*)$.\n",
    "\n",
    "Este teorema nos da mucha información acerca de la estructura de una **descomposición de autovectores** de cualquier distribución inicial $\\vec{P}(0)$. Como la base de autovectores $\\mathbf{Q}$ \n",
    "$$\n",
    "\\mathbf{Q} \\vec{x}_i = \\lambda_i \\vec{x}_i \n",
    "$$\n",
    "es una base completa, esto implica que para cualquier distribución inicial $\\vec{P}(0)$ se puede escribir como una combinación lineal\n",
    "$$\n",
    "\\vec{P}(0) = c_1 \\vec{x}_1 + c_2 \\vec{x}_2 + \\cdots + c_m \\vec{x}_m,\n",
    "$$\n",
    "con $c_1, \\cdots, c_m$ coeficientes de la expansión. Esto implica que \n",
    "\\begin{align}\n",
    "\\mathbf{Q} \\vec{P}(0) &= c_1 \\mathbf{Q} \\vec{x}_1 + c_2 \\mathbf{Q} \\vec{x}_2 + \\cdots + c_m \\mathbf{Q} \\vec{x}_m\\\\\n",
    "&= c_1 \\lambda_1 \\vec{x}_1 + c_2 \\lambda_2 \\vec{x}_2 + \\cdots + c_m \\lambda_m \\vec{x}_m.\n",
    "\\end{align}\n",
    "En general,\n",
    "$$\n",
    "\\mathbf{Q}^t \\vec{P}(0) = c_1 \\lambda_1^t \\vec{x}_1 + c_2 \\lambda_2^t \\vec{x}_2 + \\cdots + c_m \\lambda_m^t \\vec{x}_m.\n",
    "$$\n",
    "Sin embargo, sabemos que $\\lambda_1 = 1$ y $|\\lambda_i| < 1$, lo cual implica que\n",
    "$$\n",
    "\\lim_{t \\to \\infty} \\mathbf{Q}^t \\vec{P}(0) = c_1 \\vec{x}_1 \\propto \\vec{P}^*,\n",
    "$$\n",
    "donde la constante $c_1$ se escoge tal que la distribución esté normalizada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método de Metropolis-Hastings\n",
    "\n",
    "Tanto en física como en matemáticas y computación, un objetivo que aparece muy a menudo es\n",
    "\n",
    "> Construir un proceso estocástico que, al evolucionar en el tiempo, visite estados con una frecuencia proporcional a su probabilidad en la distribución deseada $\\vec{P}^*$.\n",
    "\n",
    "Esa es precisamente la idea central de los **métodos de Monte Carlo basados en cadenas de Markov**.\n",
    "\n",
    "Para realizar esta tarea, podemos utilizar el **método de Metropolis-Hastings**, el cual está desarrollado con base en la condición de **balance detallado**. Recordemos que el balance detallado es una condición suficiente para la existencia \n",
    "\n",
    "#### Algoritmo\n",
    "\n",
    "Sean $b_j$ números positivos con $j = 1, \\cdots, m$ y $B = \\sum_{j=1}^m b_j$. Supongamos que $m$ es un número muy grande y $B$ es difícil de calcular. Queremos simular una secuencia de variables aleatorias con PMF dada por un estado estacionario\n",
    "$$\n",
    "P^*_j = \\frac{b_j}{B},\\quad j = 1,\\cdots,m.\n",
    "$$\n",
    "Una forma de atacar este problema es encontrando una cadena de Markov que sea sencilla de simular y que **sus probabilidades estacionarias correspondan a $P^*_j$**.\n",
    "\n",
    "Sea $\\mathbf{Q}$ una matriz de transición perteneciente a una cadena de Markov irreducible, sobre el espacio de muestreo $1, \\cdots, m$. $Q_{ji}$ corresponde a la entrada de la fila $j$ y columna $i$ de la matriz de transición. \n",
    "\n",
    "Ahora definimos una cadena de Markov $\\{ X_n, n \\geq 0 \\}$ tal que:\n",
    "- Cuando $X_n = i$, una variable aleatoria $X$ tal que $P\\{ X = j \\} = Q_{ji}$, con $j = 1,\\cdots, m$, es generada.\n",
    "- Si $X = j$, entonces hacemos que $X_{n+1}$ sea igual a $j$ con probabilidad $\\alpha_{ij}$ o igual a $i$ con probabilidad $1 - \\alpha_{ij}$.\n",
    "- Bajo estas condiciones, la secuencia de estados generada corresponde a una cadena de Markov con probabilidades de transición $R_{ij}$, tal que\n",
    "\\begin{align}\n",
    "  R_{ij} &= Q_{ji} \\alpha_{ij},\\quad {\\textrm{si}}\\;j \\neq i \\\\\n",
    "  R_{ii} &= Q_{ii} + \\sum_{k \\neq i} Q_{ki}(1 - \\alpha_{ik})\n",
    "\\end{align}\n",
    "\n",
    "De esta forma la cadena de Markov será temporal reversible y con probabilidades $P^*_j$ si\n",
    "$$\n",
    "P^*_i R_{ij} = P^*_j R_{ji}\\quad \\forall j \\neq i,\n",
    "$$\n",
    "lo cual es equivalente a\n",
    "$$\n",
    "P^*_i Q_{ji} \\alpha_{ij} = P^*_j Q_{ij} \\alpha_{ji}.\n",
    "$$\n",
    "Se puede confirmar que si la relación anterior se satisface, entonces se debe cumplir que\n",
    "$$\n",
    "\\alpha_{ij} = {\\textrm{min}}\\Bigg( \\frac{P^*_j Q_{ij}}{P^*_i Q_{ji}}, 1 \\Bigg) = {\\textrm{min}}\\Bigg( \\frac{b_j Q_{ij}}{b_i Q_{ji}}, 1 \\Bigg).\n",
    "$$\n",
    "Note que, crucialmente, para realizar este desarrollo **no necesitamos $B$ de forma explícita ni implícita**, basta con los valores de probabilidad $b_j$.  \n",
    "\n",
    "El algoritmo anterior se puede solidificar de la siguiente forma:\n",
    "1. Escoger una matriz de transición **irreducible** $\\mathbf{Q}$ con probabilidades de transición $Q_{ji}$; $i, j = 1, \\cdots m$ y algún entero $k$ entre $1$ y $m$.\n",
    "2. Sea $n = 0$ y $X_0 = k$.\n",
    "3. Generar una variable aleatoria $X$ tal que $P\\{ X = j \\} = Q_{j X_n}$ y generar un número de una distribución uniforme $U \\in (0, 1)$\n",
    "4. Si\n",
    "   $$\n",
    "   U < \\frac{b_X Q_{X_n X}}{b_{X_n} Q_{X X_n}}\n",
    "   $$\n",
    "   Entonces $S = X$, de lo contrario, $S = X_n$\n",
    "5. Actualizar: $n = n + 1$, $X_n = S$\n",
    "6. Ir a 3. \n",
    "\n",
    "En el caso simétrico, en el cual $Q_{ij} = Q_{ji}$ (por ejemplo para un camino aleatorio uniforme), note que el paso 4. se simplifica a $U < \\frac{b_X}{b_{X_n}}$, es decir, a comparar probabilidades de la distribución estacionaria deseada. En el contexto físico, esto significa que los movimientos hacia **estados de menor energía** ($E_j < E_i$) siempre se aceptan, y los movimientos hacia **mayor energía** se aceptan con probabilidad $e^{-\\beta (E_j-E_i)}$. A esto se le conoce como el **criterio de Metropolis** (1953).\n",
    "\n",
    "#### Comentarios\n",
    "\n",
    "- La cadena resultante es **ergódica**: cualquier configuración puede alcanzarse a partir de otra con suficiente tiempo.  \n",
    "- A bajas temperaturas ($\\beta$ grande), las configuraciones tienden a alinearse (mínima energía).  \n",
    "- A altas temperaturas, el sistema se desordena (máxima entropía).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Muestreo de densidades de probabilidad - partícula unidimensional \n",
    "\n",
    "El método de Metrolois-Hastings es de **suma importancia** cuando queremos generar ciertas distribuciones de equilibrio en la Física. Su poder yace en generar cadenas de Markov que se comportan de forma ergódica y mediante esta propiedad estimar parámetros físicos.\n",
    "\n",
    "Sin embargo, para el ejemplo de esta lección veremos como generar una distribución matemática sencilla, para luego estudiar sistemas físicos más complejos.\n",
    "\n",
    "Considere una partícula unidimensional sin confinar.  \n",
    "Su densidad de probabilidad objetivo (no normalizada) es\n",
    "$$\n",
    "f(x) = e^{-x^2/2},\\qquad 0 \\le x \\le \\infty,\n",
    "$$\n",
    "Desde el punto de vista discreto, básicamente queremos \n",
    "$$\n",
    "\\vec{P}^* \\propto f(x),\n",
    "$$\n",
    "donde sabemos que el método de Metropolis-Hastings da lugar a distribuciones estacionarias módulo una constante.\n",
    "\n",
    "Aplicaremos el método para muestrear una cadena de Markov que se tiene como distribución estacionaria nuestra función objetivo. Haremos una inspección visual de la distribución obtenida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos los parámetros de nuestra simulación discreta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "L = 5.0 # longitud espacial de la grilla\n",
    "M = 101 # puntos de discretización de la grilla\n",
    "x = np.linspace(0, L, M)\n",
    "dx = x[1] - x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la versión discreta, nuestra distribución objetivo tiene la forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vals = np.exp(-0.5 * x**2)\n",
    "pi_unnorm = f_vals.copy()\n",
    "pi_discrete = pi_unnorm / np.sum(pi_unnorm) # Note que la distribución tiene que estar normalizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera etapa del algoritmo, corresponde a definir una matriz de transición $\\mathbf{Q}$ la cual tiene que pertenecer a una **cadena de Markov irreducible y aperiódica**. Naturalmente, existen muchas que podemos utilizar e, inclusive, podríamos utilizar distribuciones de probabilidad contínuas para este efecto. De hecho, lo único que necesitamos es que la distribución de probabilidad propuesta contenga una forma conocida para la probabilidad condicional $Q_{ij}$. Es común utilizar la distribución de probabilidad normal.\n",
    "\n",
    "Para efectos de nuestro primer ejemplo práctico, utilizaremos una matriz de transición discreta que sea conocida como irreducible y aperiódica. Un ejemplo corresponde a la matriz que representa un grafo en el todos los nodos tienen conexión con su nodo de la derecha e izquierda, con excepción del nodo inicial y final, que solo poseen una conexión. Esto da lugar a una matriz de transición irreducible y aperiódica, lo cual implica que posee un estado estacionario único. Es fácil visualizar que dicha matriz se implementa de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qMatrix(M):\n",
    "    Q = np.zeros((M,M))\n",
    "    for i in range(M):\n",
    "        left = i - 1 if i - 1 >= 0 else i\n",
    "        right = i + 1 if i + 1 < M else i\n",
    "        Q[i,left] += 1/3\n",
    "        Q[i,i]    += 1/3\n",
    "        Q[i,right]+= 1/3\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, para 5 nodos $(M = 5)$, tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qMatrix(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto ya tenemos lo suficiente para implemetar nuestro algortimo de Metrópolis-Hastings. **Note que nuestra matriz de transición $\\mathbf{Q}$ es simétrica**, lo cual se puede utilizar para simplificar el algoritmo.\n",
    "\n",
    "Escriba una función que simula la cadena de Markov usando el método de Metropolis-Hastings (para esto puede ser muy útil la función `np.random.choice`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente aquí el método MH usando el algoritmo de los puntos 1. a 6.\n",
    "def MetHastChain(pi, Q, N = 400_000, x0 = None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoque el algoritmo para generar una cadena\n",
    "chainx = MetHastChain(pi_unnorm, qMatrix(M))\n",
    "# Esto genera un histograma de los datos generados con la cadena\n",
    "hist_probs = np.bincount(chainx, minlength=M) / len(chainx)\n",
    "\n",
    "# Y finalmente realizamos un hisograma\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x, pi_discrete, 'k-', lw=2, label='Objetivo en la grilla discreta')\n",
    "plt.plot(x, hist_probs,  lw=1, alpha=0.9, label='Histograma MH discreto')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecuación de difusión\n",
    "\n",
    "En nuestra lección anterior, estudiamos los caminos aleatorios como un proceso estocástico. \n",
    "- En cada paso, el objeto en cuestión:\n",
    "  - se mueve a la derecha con probabilidad $p = 1/2$.  \n",
    "  - se mueve a la izquierda con probabilidad $q = 1/2$.  \n",
    "- Después de $N$ pasos, la posición es:\n",
    "  $$\n",
    "  X(N) = \\sum_{i=1}^N \\xi_i,\n",
    "  $$\n",
    "  - donde las $\\xi_i$ son variables aleatorias I.I.D. tal que $P \\{ \\xi = l \\} = \\frac{1}{2}$, $l = +1, -1$.\n",
    "\n",
    "Estudiamos los momentos de la distribución, donde obtuvimos que $E[X(N)] = 0$ y el importante resultado var$[X(N)] = N$, lo cual implica que a pesar de que *en promedio* el objeto no se desplaza, la desviación con respecto al promedio nos da un **desplazamiento característico** $\\sqrt{N}$, el cual, naturalmente cambia con el número de pasos. Si la longitud del paso es $L$, en lugar de 1, el desplazamiento característico corresponde a $L\\sqrt{N}$.\n",
    "\n",
    "Recuerde que este proceso es estocástico, lo cual implica que:\n",
    "- En general, los momentos dependen del parámetro del proceso.\n",
    "- Para nuestra derivación, usamos $N$ como parámetro; sin embargo, se puede traducir naturalmente a una **variable temporal**.\n",
    "- En teoría de probabilidad, la simulación de una variable aleatoria requiere instancias de la variable; i.e, realizar el *experimento* de crear una instancia de la variable aleatoria. Ahora, en procesos estocásticos, tenemos que una instancia corresponde a *una trayectoria*.\n",
    "\n",
    "Estudiamos el proceso estocástico de los caminos aleatorios como una versión de la distribución binomial, tal que el número de pasos a la derecha (para el caso unidimensional) $k$ está distribuido como:\n",
    "$$\n",
    "P \\{ K = k \\} = \\binom{N}{k} p^k q^{N-k},\n",
    "$$\n",
    "en el cual el desplazamiento se relaciona con $K$ via\n",
    "$$\n",
    "X(N) = 2k - N.\n",
    "$$\n",
    "Esta visión nos permitió recuperar uno de los resultados más importantes de la física estadística, **la universalidad**, la cual se manifiesta en este caso con el *teorema del límite central* para el caso de $N \\gg 1$.\n",
    "- Para $N \\gg 1$, la binomial se aproxima a una **distribución normal**:\n",
    "  $$\n",
    "  P\\{ X(N) = x \\} \\approx \\frac{1}{\\sqrt{2\\pi N}} \n",
    "  \\exp\\!\\left(-\\frac{x^2}{2N}\\right).\n",
    "  $$\n",
    "- Se demuestra aplicando la **fórmula de Stirling** a la binomial.\n",
    "\n",
    "Ahora veremos como este proceso estocástico da lugar a una ley fundamental de las ciencias: la ecuación de difusión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivación usando procesos estocásticos\n",
    "\n",
    "La ecuación de difusión se suele derivar usando continuidad, i.e., invocando la conservación de materia. En esta lección veremos como naturalmente **emerge** de la teoría de la probabilidad. \n",
    "\n",
    "Para esta derivación, usaremos expresiones con base una **regla de recursión**. Para una derivación más a-la análisis diferencial, vea la sección 2.2 del Sethna (note que hay una sutileza con respecto a cómo se define la distribución de probabilidad para el caso de variable contínua en esa derivación).\n",
    "\n",
    "Sabemos que $P\\{ X(N) = x \\}$ corresponde a la PMF de encontrar al objeto en la posición $X$ después de $N$ pasos. Recuerde que podemos escribir esta PMF de distintas formas (usando $\\xi$ o mediante la distribución binomial).\n",
    "\n",
    "Dado que en el paso $N + 1$, el objeto se mueve hacia la derecha o a la izquierda con la misma probabilidad, podemos definir una **regla de recursión**:\n",
    "$$\n",
    "P\\{ X(N + 1) = x \\} = \\tfrac{1}{2} P\\{ X(N) = x - 1 \\} + \\tfrac{1}{2} P\\{ X(N) = x + 1 \\},\n",
    "$$\n",
    "es decir; naturalmente, para que el objeto se encuentre en la posición $x$ tras $N + 1$ pasos, debe ser que tras $N$ pasos estaba en $x - 1$ o en $x + 1$, con igual probabilidad.  \n",
    "\n",
    "Ahora podemos introducir las variables de espacio y de tiempo, usando versiones discretas de estas variables.\n",
    "Tenemos:\n",
    "$$\n",
    "x = M \\,\\Delta x, \\qquad t = N \\,\\Delta t,\n",
    "$$\n",
    "donde $\\Delta x$ y $\\Delta t$ son espaciamientos discretos espaciales y temporales; mientras que $M$ y $N$ son la cantidad de pasos espaciales y temporales, respectivamente.\n",
    "- Note que esto implica que $M$ y $\\Delta x$ parametrizan el espacio, mientras que $N$ y $\\Delta t$ parametrizan el tiempo.\n",
    "- Deseamos entender **como cambia el parámetro de espacio** (el cual se refiere a la variable estocástica) **con respecto al parámetro de tiempo**.\n",
    "\n",
    "Para esto, podemos definir la **densidad de probabilidad continua**, la cual corresponde a\n",
    "$$\n",
    "\\rho(x,t) \\approx P \\{ M(N) = m \\} / \\Delta x.\n",
    "$$\n",
    "La idea corresponde a que la aproximación corresponde a una igualdad en el límite $\\Delta x, \\Delta t \\to 0$; i.e., **el límite contínuo**. El límite continuo puede tener sutilezas en algunas instancias de la física estadística, pero en este caso, podemos asumir que en general $\\rho(x, t)$ es una función analítica.\n",
    "- Note que solamente la posición es una variable estocástica, el tiempo corresponde a su parámetro.\n",
    "\n",
    "Considerando $\\Delta t$ como un parámetro pequeño, podemos hacer una expansión del parámetro temporal.\n",
    "- Aquí es donde yace la sutileza...\n",
    "- ¿Que tan pequeño tiene que ser $\\Delta t$ y, por ende, $\\Delta x$?\n",
    "- La pregunta a esta respuesta es más sencilla de responder *a posteriori*. $\\Delta t$ y $\\Delta x$ deben ser lo suficientemente pequeños, tal que sobre ese rango $\\rho(x, t)$ *no varía apreciablemente*. En términos físicos, esto implica que $\\rho(x, t)$ debe ser una función bien comportada. En términos matemáticos, esto implica analiticidad. \n",
    "\n",
    "Habiendo hecho los *disclaimers*, procedemos a rescribir la regla de recursión en términos de $\\rho(x, t)$:\n",
    "$$\n",
    "\\rho(x,t+\\Delta t) = \\tfrac{1}{2} \\, \\rho(x-\\Delta x, t) + \\tfrac{1}{2}\\, \\rho(x+\\Delta x,t)\n",
    "$$\n",
    "y procedemos a realizar una expansión alrededor de $\\rho(x, t)$.\n",
    "- Expansión temporal (lado izquiedo):\n",
    "$$\n",
    "\\rho(x,t+\\Delta t) = \\rho(x,t) + \\Delta t \\,\\frac{\\partial \\rho(x, t)}{\\partial t} + \\frac{(\\Delta t)^2}{2}\\frac{\\partial^2 \\rho(x, t))}{\\partial t^2} + O[(\\Delta t)^3]\n",
    "$$\n",
    "\n",
    "- Expansión espacial (lado derecho):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tfrac{1}{2} \\rho(x-\\Delta x,t) + \\tfrac{1}{2} \\rho(x+\\Delta x,t) \n",
    "&= \\tfrac{1}{2}\\Big[\\rho(x,t) - \\Delta x \\frac{\\partial \\rho(x, t)}{\\partial x} + \\frac{(\\Delta x)^2}{2}\\frac{\\partial^2 \\rho(x, t)}{\\partial x^2} + O[(\\Delta x)^3]\\Big] \\\\\n",
    "&\\quad + \\tfrac{1}{2}\\Big[\\rho(x,t) + \\Delta x \\frac{\\partial \\rho(x, t)}{\\partial x} + \\frac{(\\Delta x)^2}{2}\\frac{\\partial^2 \\rho(x, t)}{\\partial x^2} + O[(\\Delta x)^3]\\Big] \\\\\n",
    "&= \\rho(x,t) + \\frac{(\\Delta x)^2}{2} \\frac{\\partial^2 \\rho}{\\partial x^2}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Igualando ambos lados:\n",
    "$$\n",
    "\\rho(x,t) + \\Delta t \\,\\frac{\\partial \\rho(x, t)}{\\partial t} + \\frac{(\\Delta t)^2}{2}\\frac{\\partial^2 \\rho(x, t))}{\\partial t^2} \\;\\approx\\; \\rho(x,t) + \\frac{(\\Delta x)^2}{2}\\frac{\\partial^2 \\rho(x, t)}{\\partial x^2}.\n",
    "$$\n",
    "Ahora debemos decidir cuáles términos mantener. Para esto debemos establecer el límite contínuo:\n",
    "- En el límite contínuo, hacemos $\\Delta t \\to 0$ **y** $\\Delta x \\to 0$, **al mismo tiempo**.\n",
    "- Es decir, no podemos tomar un límite primero que el otro, o esto resultaría en una ecuación contínua trivial.\n",
    "- Esto implica, que el límite continuo se debe tomar de la siguiente forma:\n",
    "$$\n",
    "\\frac{(\\Delta x)^2}{\\Delta t} \\to {\\textrm{const.}}\n",
    "$$\n",
    "- De caso contrario, la expansión no da lugar a una ley de variación de $\\rho(x, t)$.\n",
    "- La razón por la cual se escoge el espaciamiento $(\\Delta x)^2$ en su orden cuadrático y $\\Delta t$ en su orden lineal, es porque el término lineal $\\Delta x$ desaparece, y el orden cuadrático $(\\Delta x)^2$ es el orden más alto de la expansión.\n",
    "\n",
    "Finalmente, escogiendo solo los términos de mayor orden:\n",
    "$$\n",
    "\\frac{\\partial \\rho(x, t)}{\\partial t} = \\frac{(\\Delta x)^2}{2 \\Delta t}\\frac{\\partial^2 \\rho(x, t)}{\\partial x^2}.\n",
    "$$\n",
    "\n",
    "La ecuación de difusión se escribe definiendo un valor para la constante anterior, la cual llamamos **coeficiente de difusión** en el límite continuo:\n",
    "$$\n",
    "D = \\frac{(\\Delta x)^2}{2 \\Delta t}.\n",
    "$$\n",
    "En este límite,\n",
    "$$\n",
    "\\boxed{\\;\\;\\frac{\\partial \\rho(x, t)}{\\partial t} = D \\frac{\\partial^2 \\rho(x, t)}{\\partial x^2}\\;\\;}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrientes y fuerzas externas\n",
    "\n",
    "Nuestra derivación nos permite hablar de **corrientes de cantidades conservadas**.\n",
    "\n",
    "Sea $\\rho(x, t)$ la densidad de alguna cantidad que se conserva (partículas, número de moléculas, masa, energía, probabilidad, etc) que varía en la dirección $x$. Definamos $J(x, t)$ como la tasa sobre la cual esta cantidad pasa a través de una región denotada entre $x$ y $x + \\Delta x$ a un tiempo $t$. \n",
    "\n",
    "Partiendo de la ecuación de difusión\n",
    "$$\n",
    "\\frac{\\partial \\rho(x,t)}{\\partial t} = D \\frac{\\partial^2 \\rho(x,t)}{\\partial x^2},\n",
    "$$\n",
    "podemos reescribir la ecuación de difusión usando $J(x, t)$ \n",
    "$$\n",
    "\\frac{\\partial \\rho(x, t)}{\\partial t} + \\frac{\\partial J(x, t)}{\\partial x} = 0,\n",
    "$$\n",
    "donde\n",
    "$$\n",
    "J(x,t) = -D \\frac{\\partial \\rho(x, t)}{\\partial x}.\n",
    "$$\n",
    "Las ecuaciones anteriores definen **una ley de conservación**. \n",
    "- Notemos el *importante* símbolo negativo en la definición de la corriente, lo cual implica que el flujo ocurre **opuesto al grandiente de la densidad de probabilidad**, es decir, de regiones de más probabilidad a menos probabilidad.\n",
    "- Este tipo de leyes fundamentales son ubicuas en las ciencias, por ejemplo:\n",
    "    - Ley de Fick (difusión de partículas): El flujo es proporcional al gradiente de densidad.\n",
    "    - Ley de Ohm (electricidad): corriente proporcional al gradiente de potencial eléctrico.\n",
    "    - Conducción de calor: flujo de calor proporcional al gradiente de temperatura.\n",
    "\n",
    "Todas estas leyes son ejemplos de **respuesta lineal**: el flujo $J$ es proporcional a un gradiente (la “fuerza termodinámica”).\n",
    "\n",
    "Si además de la difusión actuara una **fuerza externa** $F(x)$, los objetos heredan una **velocidad de arrastre promedio** proporcional a la fuerza:  \n",
    "$$\n",
    "v(x) = \\gamma F(x),\n",
    "$$\n",
    "donde $\\gamma$ es una constante conocida como movilidad. Desde el punto de vista microscópico, esto ocurre si existe un **sesgo** en el camino aleatorio, es decir, una fuerza que conlleva a los objetos a moverse con mayor probabilidad a un lado que a otro. Esto genera un valor de expectación distinto de zero para cada paso.\n",
    "\n",
    "Entonces la corriente tiene dos contribuciones:\n",
    "$$\n",
    "J(x,t) = -D \\frac{\\partial \\rho(x, t)}{\\partial x} + \\mu F(x) \\rho(x,t).\n",
    "$$\n",
    "\n",
    "- Primer término: corriente difusiva (estocástica).  \n",
    "- Segundo término: corriente de arrastre (determinística).  \n",
    "\n",
    "Para obtener la ecuación de difusión resultante, sustituimos en la ecuación de continuidad:\n",
    "$$\n",
    "\\frac{\\partial \\rho(x, t)}{\\partial t} = D \\frac{\\partial^2 \\rho(x, t)}{\\partial x^2} - \\frac{\\partial}{\\partial x}\\big( \\mu F(x) \\rho(x, t) \\big).\n",
    "$$\n",
    "\n",
    "En la literatura, se le suele llamar a esta ecuación como una **ecuación de Fokker–Planck** (para el caso simple 1D).\n",
    "- La **difusión** describe la emergencia del orden macroscópico a raíz del desorden microscópico (fluctuaciones térmicas).  \n",
    "- La **fuerza externa** introduce sesgo o direccionalidad.  \n",
    "- El equilibrio se alcanza cuando la corriente total es cero en tiempos largos ($t^*$), es decir, cuando la densidad de probabilidad **no cambia con el tiempo**:  \n",
    "$$\n",
    "J(x, t^*) = 0 \\quad \\Rightarrow \\quad \\rho_{\\text{eq}}(x, t^*) \\propto e^{-U(x)/k_B T}, \n",
    "$$\n",
    "con $U(x)$ el potencial asociado a $F(x) = -\\partial U(x) / \\partial x$.\n",
    "\n",
    "Esto es la emergencia del concepto de **ergodicidad**, que estudiaremos en procesos de Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conexión con el concepto de entropía\n",
    "\n",
    "En física estadística, la entropía se refiere al número de estados microscópicos accesibles. Si hay pocos estados accesibles, la entropía es baja y si lo contrario ocurre, la entropía es alta.\n",
    "- El sistema tiende a homogeneizarse.\n",
    "- La distribución más probable es aquella donde las partículas están uniformemente repartidas.\n",
    "- Difundir hacia regiones de menor densidad aumenta el número de configuraciones posibles, es decir, la entropía."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mayores dimensiones\n",
    "\n",
    "El proceso estocástico que hemos estudiado se puede generalizar a $d$ dimensiones. \n",
    "- En cada paso, la partícula elige al azar moverse en alguna dirección.  \n",
    "- La posición después de $N$ pasos se define con un vector aleatorio:\n",
    "$$\n",
    "\\mathbf{\\vec{X}}(N) = (X_1, X_2, \\cdots, X_d).\n",
    "$$\n",
    "Al igual que hicimos para el caso 1D, podemos reescribir\n",
    "$$\n",
    "\\mathbf{\\vec{X}}(N) = \\sum_{i=1}^N \\mathbf{\\vec{\\xi}}_i,\n",
    "$$\n",
    "con $\\mathbf{\\vec{\\xi}}_i$ un vector de paso elegido al azar. Podemos definir una **matriz de covarianza** como\n",
    "$$\n",
    "\\Sigma_{ij} = E[X_i X_j] - E[X_i]E[X_j].\n",
    "$$\n",
    "La variable aleatoria para el $i$-ésimo paso $\\mathbf{\\vec{\\xi}}_i$ tiene $d$ componentes independientes en las distintas direcciones. \n",
    "Para un paso de longitud fija $\\ell$, la norma del $i$-ésimo paso es\n",
    "$$\n",
    "|\\mathbf{\\vec{\\xi}}_i|^2 = \\sum_{k=1}^d \\mathbf{\\xi}_{i, k}^2 = \\ell^2,\n",
    "$$\n",
    "donde hemos usado el índice $k$ para indicar la dirección y el índice $i$ para denotar la variable aleatoria de cada paso.\n",
    "Tenemos que\n",
    "$$\n",
    "E[\\mathbf{\\xi}_{i, k}] = 0,\n",
    "$$\n",
    "lo cual implica que\n",
    "$$\n",
    "E[\\mathbf{\\vec{\\xi}}_{i}] = 0.\n",
    "$$\n",
    "Para la varianza, dado que el camino aleatorio es **uniforme** (simétrico), cada componente tiene la misma varianza:\n",
    "$$\n",
    "{\\textrm{var}}[\\mathbf{\\xi}_{i, 1}] = {\\textrm{var}}[\\mathbf{\\xi}_{i, 2}] = \\cdots = {\\textrm{var}}[\\mathbf{\\xi}_{i, d}]\n",
    "$$\n",
    "lo cual implica que la sumar en todas las direcciones obtenemos:\n",
    "$$\n",
    "\\sum_{k=1}^d {\\textrm{var}}[\\mathbf{\\xi}_{i, k}] = \\sum_{k=1}^d \\mathbf{\\xi}^2_{i, k} = \\ell^2.\n",
    "$$\n",
    "Como todas las varianzas son iguales:\n",
    "$$\n",
    "\\sum_{k=1}^d {\\textrm{var}}[\\mathbf{\\xi}_{i, k}] = d \\cdot {\\textrm{var}}[\\mathbf{\\xi}_{i, k}] = \\ell^2 \n",
    "$$\n",
    "lo cual implica\n",
    "$$\n",
    "{\\textrm{var}}[\\mathbf{\\xi}_{i, k}] = \\frac{\\ell^2}{d}.\n",
    "$$\n",
    "La varianza de cada componente independiente es igual que en el caso 1D, solamente que en este caso se \"reparte\" en $d$ direcciones. Por independencia de las variables aleatorias, no hay correlaciones en diferentes direcciones:\n",
    "$$\n",
    "E[\\mathbf{\\xi}_{i,k} \\mathbf{\\xi}_{i,m}] = 0 \\quad \\text{para } k \\neq m.\n",
    "$$\n",
    "Ahora nos falta sumar sobre todos los $N$ pasos del camino aleatorio.\n",
    "Dado que\n",
    "$$\n",
    "\\mathbf{\\vec{X}}(N) = \\sum_{i=1}^N \\mathbf{\\vec{\\xi}}_{i},\n",
    "$$\n",
    "entonces\n",
    "$$\n",
    "\\Sigma_{ij} = \\sum_{n=1}^N E[ \\mathbf{\\xi}_{n,i} \\mathbf{\\xi}_{n,j} ].\n",
    "$$\n",
    "Por independencia entre las variables aleatorias de los pasos:\n",
    "$$\n",
    "\\Sigma_{ij} = N E[ \\mathbf{\\xi}_{1,i} \\mathbf{\\xi}_{1,j} ].\n",
    "$$\n",
    "Basta esto para obtener el resultado final:\n",
    "- Para $i = j$:\n",
    "  $$\n",
    "  \\Sigma_{ii} = N \\frac{\\ell^2}{d}.\n",
    "  $$\n",
    "- Para $i \\neq j$:\n",
    "  $$\n",
    "  \\Sigma_{ij} = 0.\n",
    "  $$\n",
    "\n",
    "Lo cual implica que la matriz de covarianza es **diagonal e isotrópica**:\n",
    "$$\n",
    "\\Sigma = \\frac{N \\ell^2}{d} \\, I_d,\n",
    "$$\n",
    "donde $I_d$ es la matriz identidad de dimensión $d \\times d$. La traza de esta matriz nos da **la varianza total del dezplazamiento con respecto al origen**:\n",
    "$$\n",
    "{\\textrm{var}}[\\mathbf{\\vec{X}}(N)] = \\mathrm{Tr}(\\Sigma) = N \\ell^2.\n",
    "$$\n",
    "Al igual que en 1D, la raíz cuadrática media del desplazamiento crece como $\\sqrt{N}$, con la diferencia de que ahora se reparte de forma simétrica en todas las direcciones.\n",
    "\n",
    "Ahora podemos tomar el límite continuo de forma apropiada, en el cual definimos el tiempo como $t = N \\Delta t$ y el coeficiente de difusión es\n",
    "$$\n",
    "D = \\frac{\\ell^2}{2d \\,\\Delta t},\n",
    "$$\n",
    "con el se obtiene la **ecuación de difusión** en $d$ dimensiones:\n",
    "$$\n",
    "\\frac{\\partial \\rho(\\mathbf{\\vec{x}},t)}{\\partial t} = D \\nabla^2 \\rho(\\mathbf{\\vec{x}},t).\n",
    "$$\n",
    "Note que el coeficiente de disfusión depende de $d$.\n",
    "\n",
    "Las corrientes conservadas también se generalizan de forma directa.\n",
    "- La ecuación de difusión puede escribirse como una ecuación de continuidad:\n",
    "$$\n",
    "\\frac{\\partial \\rho(\\mathbf{\\vec{x}},t)}{\\partial t} + \\nabla \\cdot J(\\mathbf{\\vec{x}},t) = 0.\n",
    "$$\n",
    "- Donde la **corriente de probabilidad** es:\n",
    "$$\n",
    "J(\\mathbf{\\vec{x}},t) = -D \\nabla \\rho(\\mathbf{\\vec{x}},t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soluciones computacionales 1 (diferencias finitas)\n",
    "\n",
    "Las soluciones de la ecuación de difusión se pueden estudiar desde distintas perspectivas. En la próxima lección haremos un análisis detallado que nos dará una intución física de las soluciones a luz de la perspectiva estocástica y la universalidad.\n",
    "\n",
    "Para observar soluciones desde la perspectiva computacional, podemos utilizar diferencias finitas.\n",
    "\n",
    "**Objetivo:** resolver numéricamente\n",
    "$$\n",
    "\\frac{\\partial \\rho(x,t)}{\\partial t} = D \\frac{\\partial^2 \\rho(x, t)}{\\partial x^2}\n",
    "$$\n",
    "con FTCS hasta un tiempo observado $t_{\\mathrm{obs}}$ y comparar $\\rho_{\\mathrm{num}}(x,t_{\\mathrm{obs}})$ con la solución analítica (¡note que ya obtuvimos la solución usando el teorema del límite central! La veremos más formalmente en la próxima lección)\n",
    "$$\n",
    "\\rho_{\\mathrm{an}}(x,t) = \\frac{1}{\\sqrt{4\\pi D t}} \\exp\\!\\Big(-\\frac{x^2}{4Dt}\\Big).\n",
    "$$\n",
    "\n",
    "**Esquema:** (*forward time centred space*)\n",
    "$$\n",
    "\\rho_i^{n+1} = \\rho_i^n + r\\,(\\rho_{i+1}^n - 2\\rho_i^n + \\rho_{i-1}^n),\\qquad r=\\frac{D\\Delta t}{(\\Delta x)^2}.\n",
    "$$\n",
    "el superíndice $n$ se refiere a la discretización temporal, mientras que el subíndice $i$ se refiere a la discretización de la coordenada espacial.\n",
    "\n",
    "**Condiciones:**\n",
    "- Dominio $x\\in[-L,L]$\n",
    "- Condiciones de frontera Dirichlet $\\rho(\\pm L, t) = 0$ (o suficientemente lejos)\n",
    "- Condición inicial: delta de Dirac, aproximada por una Gaussiana normalizada muy estrecha\n",
    "\n",
    "Comprobaremos invarianza de la masa y calcularemos el error $L^2$:\n",
    "$$\n",
    "\\|e\\|_2 = \\sqrt{\\int (\\rho_{\\mathrm{num}}-\\rho_{\\mathrm{an}})^2 \\, dx} \\approx \\sqrt{\\sum_i (\\rho_i^{\\text{num}}-\\rho_i^{\\text{an}})^2 \\Delta x }.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dado que estamos en un ambiente integrado, podemos usar variables globales\n",
    "# Definamos los parámetros de la simulación\n",
    "# Juegue con estos parámetros:\n",
    "dDifusion = 1.0 # constante de difusión\n",
    "L = 40 # longitud de la grilla espacial (en unidades espaciales)\n",
    "dx = 0.2 # espaciamiento espacial\n",
    "dt = 0.005 # espaciamiento temporal\n",
    "r = dDifusion * dt / (dx**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NumPy` nos permite crear una grilla espacial de forma muy sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-L, L+dx, dx)\n",
    "mPoints = x.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos establecer una condición inicial. El beneficio de los métodos computacionales es que podemos implementar cualquier condición inicial deseada. A manera de ejemplo, usemos una delta de Dirac situada en el medio; i.e., la distribución inicial de objetos concentrada en un punto. A nivel computacional, podemos aproximar esta condición inicial con una distribución Gaussiana muy estrecha. Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma0 = 0.3\n",
    "p0 = np.exp(-x**2 / (2*sigma0**2))\n",
    "p0 = p0 / (p0.sum() * dx)   # La normalización se toma de esta forma por ser una grilla discreta\n",
    "                            # El dx se requiere para normalizar la \"integral\" a 1 (entre comillas, dado que sería más una suma\n",
    "                            # de Riemann que una integral continua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro análisis, requerimos un arreglo que corresponde a la grilla en el punto temporal actual $n$ y otro para la grilla en el punto temporal $n+1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p0.copy()\n",
    "p_new = np.zeros_like(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos el tiempo máximo para el cuál queremos la solución y la cantidad de iteraciones necesarias para llegar a ese tiempo usando pasos temporales de tamaño `dt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_obs = 1.0 # juegue con este parámetro\n",
    "nTimes = int(np.ceil(t_obs / dt)) # A lo sumo, esto será una iteración más después del valor deseado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos a implementar el método FTCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.0\n",
    "for n in range(nTimes):\n",
    "    # Implemente aquí la iteración requerida para evolucionar el sistema, recuerde las condiciones de fontera!\n",
    "    # Su código aquí: Use p_new para la solución al futuro y p para la solución al tiempo actual\n",
    "    # Ahora hacemos el cambio p_new <-> p\n",
    "    # Su código aquí\n",
    "    t += dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conocemos la solución analítica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan = (1.0 / np.sqrt(4 * np.pi * dDifusion * t)) * np.exp(-x**2 / (4 * dDifusion * t))\n",
    "pan = pan / (pan.sum() * dx)   # normalizar de la misma forma a la anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realice una gráfico de la solución numérica vs la solución analítica. Estime el error usando la norma L2 descrita anteriormente."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
